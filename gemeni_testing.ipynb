{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'google.genai' has no attribute 'configure'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m genai\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m types\n\u001b[1;32m----> 4\u001b[0m \u001b[43mgenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigure\u001b[49m(api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGEMINI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mGenerativeModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-pro\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate_content(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow does AI work?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'google.genai' has no attribute 'configure'"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "genai.configure(api_key=\"GEMINI_API_KEY\")\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-pro\")\n",
    "response = model.generate_content(\"How does AI work?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a large language model, trained by Google.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash-exp', contents='What is your name?'\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "model=genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash\",\n",
    "  system_instruction=\"You are a cat. Your name is Neko.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunbeam naps are essential.  Tuna?  Where's the tuna?  Humans are mostly okay... for scratching.  Boxes.  Always boxes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\"Explain your life in 5 phrases.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GenerativeModel in module google.generativeai.generative_models:\n",
      "\n",
      "class GenerativeModel(builtins.object)\n",
      " |  GenerativeModel(model_name: 'str' = 'gemini-1.5-flash-002', safety_settings: 'safety_types.SafetySettingOptions | None' = None, generation_config: 'generation_types.GenerationConfigType | None' = None, tools: 'content_types.FunctionLibraryType | None' = None, tool_config: 'content_types.ToolConfigType | None' = None, system_instruction: 'content_types.ContentType | None' = None)\n",
      " |  \n",
      " |  The `genai.GenerativeModel` class wraps default parameters for calls to\n",
      " |  `GenerativeModel.generate_content`, `GenerativeModel.count_tokens`, and\n",
      " |  `GenerativeModel.start_chat`.\n",
      " |  \n",
      " |  This family of functionality is designed to support multi-turn conversations, and multimodal\n",
      " |  requests. What media-types are supported for input and output is model-dependant.\n",
      " |  \n",
      " |  >>> import google.generativeai as genai\n",
      " |  >>> import PIL.Image\n",
      " |  >>> genai.configure(api_key='YOUR_API_KEY')\n",
      " |  >>> model = genai.GenerativeModel('models/gemini-pro')\n",
      " |  >>> result = model.generate_content('Tell me a story about a magic backpack')\n",
      " |  >>> result.text\n",
      " |  \"In the quaint little town of Lakeside, there lived a young girl named Lily...\"\n",
      " |  \n",
      " |  Multimodal input:\n",
      " |  \n",
      " |  >>> model = genai.GenerativeModel('models/gemini-pro')\n",
      " |  >>> result = model.generate_content([\n",
      " |  ...     \"Give me a recipe for these:\", PIL.Image.open('scones.jpeg')])\n",
      " |  >>> result.text\n",
      " |  \"**Blueberry Scones** ...\"\n",
      " |  \n",
      " |  Multi-turn conversation:\n",
      " |  \n",
      " |  >>> chat = model.start_chat()\n",
      " |  >>> response = chat.send_message(\"Hi, I have some questions for you.\")\n",
      " |  >>> response.text\n",
      " |  \"Sure, I'll do my best to answer your questions...\"\n",
      " |  \n",
      " |  To list the compatible model names use:\n",
      " |  \n",
      " |  >>> for m in genai.list_models():\n",
      " |  ...     if 'generateContent' in m.supported_generation_methods:\n",
      " |  ...         print(m.name)\n",
      " |  \n",
      " |  Arguments:\n",
      " |       model_name: The name of the model to query. To list compatible models use\n",
      " |       safety_settings: Sets the default safety filters. This controls which content is blocked\n",
      " |           by the api before being returned.\n",
      " |       generation_config: A `genai.GenerationConfig` setting the default generation parameters to\n",
      " |           use.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, model_name: 'str' = 'gemini-1.5-flash-002', safety_settings: 'safety_types.SafetySettingOptions | None' = None, generation_config: 'generation_types.GenerationConfigType | None' = None, tools: 'content_types.FunctionLibraryType | None' = None, tool_config: 'content_types.ToolConfigType | None' = None, system_instruction: 'content_types.ContentType | None' = None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__ = __str__(self)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  count_tokens(self, contents: 'content_types.ContentsType' = None, *, generation_config: 'generation_types.GenerationConfigType | None' = None, safety_settings: 'safety_types.SafetySettingOptions | None' = None, tools: 'content_types.FunctionLibraryType | None' = None, tool_config: 'content_types.ToolConfigType | None' = None, request_options: 'helper_types.RequestOptionsType | None' = None) -> 'protos.CountTokensResponse'\n",
      " |      # fmt: off\n",
      " |  \n",
      " |  async count_tokens_async(self, contents: 'content_types.ContentsType' = None, *, generation_config: 'generation_types.GenerationConfigType | None' = None, safety_settings: 'safety_types.SafetySettingOptions | None' = None, tools: 'content_types.FunctionLibraryType | None' = None, tool_config: 'content_types.ToolConfigType | None' = None, request_options: 'helper_types.RequestOptionsType | None' = None) -> 'protos.CountTokensResponse'\n",
      " |  \n",
      " |  generate_content(self, contents: 'content_types.ContentsType', *, generation_config: 'generation_types.GenerationConfigType | None' = None, safety_settings: 'safety_types.SafetySettingOptions | None' = None, stream: 'bool' = False, tools: 'content_types.FunctionLibraryType | None' = None, tool_config: 'content_types.ToolConfigType | None' = None, request_options: 'helper_types.RequestOptionsType | None' = None) -> 'generation_types.GenerateContentResponse'\n",
      " |      A multipurpose function to generate responses from the model.\n",
      " |      \n",
      " |      This `GenerativeModel.generate_content` method can handle multimodal input, and multi-turn\n",
      " |      conversations.\n",
      " |      \n",
      " |      >>> model = genai.GenerativeModel('models/gemini-pro')\n",
      " |      >>> response = model.generate_content('Tell me a story about a magic backpack')\n",
      " |      >>> response.text\n",
      " |      \n",
      " |      ### Streaming\n",
      " |      \n",
      " |      This method supports streaming with the `stream=True`. The result has the same type as the non streaming case,\n",
      " |      but you can iterate over the response chunks as they become available:\n",
      " |      \n",
      " |      >>> response = model.generate_content('Tell me a story about a magic backpack', stream=True)\n",
      " |      >>> for chunk in response:\n",
      " |      ...   print(chunk.text)\n",
      " |      \n",
      " |      ### Multi-turn\n",
      " |      \n",
      " |      This method supports multi-turn chats but is **stateless**: the entire conversation history needs to be sent with each\n",
      " |      request. This takes some manual management but gives you complete control:\n",
      " |      \n",
      " |      >>> messages = [{'role':'user', 'parts': ['hello']}]\n",
      " |      >>> response = model.generate_content(messages) # \"Hello, how can I help\"\n",
      " |      >>> messages.append(response.candidates[0].content)\n",
      " |      >>> messages.append({'role':'user', 'parts': ['How does quantum physics work?']})\n",
      " |      >>> response = model.generate_content(messages)\n",
      " |      \n",
      " |      For a simpler multi-turn interface see `GenerativeModel.start_chat`.\n",
      " |      \n",
      " |      ### Input type flexibility\n",
      " |      \n",
      " |      While the underlying API strictly expects a `list[protos.Content]` objects, this method\n",
      " |      will convert the user input into the correct type. The hierarchy of types that can be\n",
      " |      converted is below. Any of these objects can be passed as an equivalent `dict`.\n",
      " |      \n",
      " |      * `Iterable[protos.Content]`\n",
      " |      * `protos.Content`\n",
      " |      * `Iterable[protos.Part]`\n",
      " |      * `protos.Part`\n",
      " |      * `str`, `Image`, or `protos.Blob`\n",
      " |      \n",
      " |      In an `Iterable[protos.Content]` each `content` is a separate message.\n",
      " |      But note that an `Iterable[protos.Part]` is taken as the parts of a single message.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          contents: The contents serving as the model's prompt.\n",
      " |          generation_config: Overrides for the model's generation config.\n",
      " |          safety_settings: Overrides for the model's safety settings.\n",
      " |          stream: If True, yield response chunks as they are generated.\n",
      " |          tools: `protos.Tools` more info coming soon.\n",
      " |          request_options: Options for the request.\n",
      " |  \n",
      " |  async generate_content_async(self, contents: 'content_types.ContentsType', *, generation_config: 'generation_types.GenerationConfigType | None' = None, safety_settings: 'safety_types.SafetySettingOptions | None' = None, stream: 'bool' = False, tools: 'content_types.FunctionLibraryType | None' = None, tool_config: 'content_types.ToolConfigType | None' = None, request_options: 'helper_types.RequestOptionsType | None' = None) -> 'generation_types.AsyncGenerateContentResponse'\n",
      " |      The async version of `GenerativeModel.generate_content`.\n",
      " |  \n",
      " |  start_chat(self, *, history: 'Iterable[content_types.StrictContentType] | None' = None, enable_automatic_function_calling: 'bool' = False) -> 'ChatSession'\n",
      " |      Returns a `genai.ChatSession` attached to this model.\n",
      " |      \n",
      " |      >>> model = genai.GenerativeModel()\n",
      " |      >>> chat = model.start_chat(history=[...])\n",
      " |      >>> response = chat.send_message(\"Hello?\")\n",
      " |      \n",
      " |      Arguments:\n",
      " |          history: An iterable of `protos.Content` objects, or equivalents to initialize the session.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_cached_content(cached_content: 'str | caching.CachedContent', *, generation_config: 'generation_types.GenerationConfigType | None' = None, safety_settings: 'safety_types.SafetySettingOptions | None' = None) -> 'GenerativeModel'\n",
      " |      Creates a model with `cached_content` as model's context.\n",
      " |      \n",
      " |      Args:\n",
      " |          cached_content: context for the model.\n",
      " |          generation_config: Overrides for the model's generation config.\n",
      " |          safety_settings: Overrides for the model's safety settings.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `GenerativeModel` object with `cached_content` as its context.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  cached_content\n",
      " |  \n",
      " |  model_name\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(genai.GenerativeModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Sunbeam naps are mandatory.  Tuna?  Where's the tuna?  Humans are okay... sometimes.  Boxes are the ultimate comfort.\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"avg_logprobs\": -0.4006096624558972\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 20,\n",
       "        \"candidates_token_count\": 31,\n",
       "        \"total_token_count\": 51\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phishing-analysis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
